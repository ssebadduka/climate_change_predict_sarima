{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 636393,
          "sourceType": "datasetVersion",
          "datasetId": 312121
        }
      ],
      "dockerImageVersionId": 30203,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Climate Prediction",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssebadduka/climate_change_predict_sarima/blob/main/Climate_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'daily-climate-time-series-data:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F312121%2F636393%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240806%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240806T094247Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D454b686c3aae1aad305f137ae627f82214aa9a371157af7ad0564ef29e914d87e7cf06bc3a942777c6b3858da1c02389cca998b8c17920f75636f5484ac26e1c9a9a451d002efe2e3effb4736eb69dbfda3717cf96109c27058b9907a90bc6e2e3029439443669cd46dd7d670d1f915b51168fafb2a4fcb097b68783d1f513af23075a0afd0fa2da4ae46a3677be595e9c7841ca95b9b669e55e230141e7eb0f9ed51ccd2941f6af42a24f0c692ccbc698ea60dd109dfc5ebf86cbc29e6b0bb765ccaa339484f1c937b464c5c2a32e2f929d00e1afc7201314859d6f76b0ccb8d6ddb33d7d36d5e87e0b03cda311aa22d60e9d8ebbe88528b8ec2ac565a41a37'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "hUmAONomNtiw"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras\n",
        "from tensorflow import nn"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:05:57.859772Z",
          "iopub.execute_input": "2022-06-30T14:05:57.860442Z",
          "iopub.status.idle": "2022-06-30T14:06:08.796727Z",
          "shell.execute_reply.started": "2022-06-30T14:05:57.860344Z",
          "shell.execute_reply": "2022-06-30T14:06:08.795856Z"
        },
        "trusted": true,
        "id": "5-5WOK6pNti8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardizing the size of all images\n",
        "mpl.rcParams['figure.figsize'] = (15,9)\n",
        "mpl.rcParams['font.size'] = 15"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:06:08.798311Z",
          "iopub.execute_input": "2022-06-30T14:06:08.799052Z",
          "iopub.status.idle": "2022-06-30T14:06:08.803054Z",
          "shell.execute_reply.started": "2022-06-30T14:06:08.79902Z",
          "shell.execute_reply": "2022-06-30T14:06:08.80215Z"
        },
        "trusted": true,
        "id": "-3akPBaINti-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Exploration"
      ],
      "metadata": {
        "id": "bQvA5HprNtjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this dataset, we have one dataset for training and another one for test. To simplify things, we gonna merge both of them in just one, this will make things easier to explore and manipulate the data"
      ],
      "metadata": {
        "id": "KBabTngGNtjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('../input/daily-climate-time-series-data/DailyDelhiClimateTrain.csv')\n",
        "df_train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:06:08.804051Z",
          "iopub.execute_input": "2022-06-30T14:06:08.804341Z",
          "iopub.status.idle": "2022-06-30T14:06:08.903685Z",
          "shell.execute_reply.started": "2022-06-30T14:06:08.804314Z",
          "shell.execute_reply": "2022-06-30T14:06:08.902532Z"
        },
        "trusted": true,
        "id": "3aWAqs98NtjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('../input/daily-climate-time-series-data/DailyDelhiClimateTest.csv')\n",
        "df_test.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:06:08.906192Z",
          "iopub.execute_input": "2022-06-30T14:06:08.906531Z",
          "iopub.status.idle": "2022-06-30T14:06:08.92926Z",
          "shell.execute_reply.started": "2022-06-30T14:06:08.906501Z",
          "shell.execute_reply": "2022-06-30T14:06:08.928444Z"
        },
        "trusted": true,
        "id": "b_GZUd_sNtjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the datasets in one\n",
        "df = pd.concat((df_test, df_train), ignore_index=True)\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df.sort_values(by=['date'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:06:08.930461Z",
          "iopub.execute_input": "2022-06-30T14:06:08.930916Z",
          "iopub.status.idle": "2022-06-30T14:06:08.948398Z",
          "shell.execute_reply.started": "2022-06-30T14:06:08.930886Z",
          "shell.execute_reply": "2022-06-30T14:06:08.947533Z"
        },
        "trusted": true,
        "id": "3MMUF9RONtjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the behavior of all columns\n",
        "columns = df.drop(columns=['date']).columns\n",
        "for column in columns:\n",
        "    sns.lineplot(x=df['date'], y=df[column])\n",
        "    plt.title(column)\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:06:08.949563Z",
          "iopub.execute_input": "2022-06-30T14:06:08.950133Z",
          "iopub.status.idle": "2022-06-30T14:06:10.450668Z",
          "shell.execute_reply.started": "2022-06-30T14:06:08.950103Z",
          "shell.execute_reply": "2022-06-30T14:06:10.449436Z"
        },
        "trusted": true,
        "id": "19hnUutkNtjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the columns look very stable, so this is good for us and our models!"
      ],
      "metadata": {
        "id": "eRZBaKLDNtjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert our dataset in a time series dataset\n",
        "def create_window(target, feature, window=1, offset=0):\n",
        "    feature_new, target_new = [], []\n",
        "    feature_np = feature.to_numpy()\n",
        "    target_np = target.to_numpy()\n",
        "    for i in range(window, target.shape[0] - offset):\n",
        "        feature_list = feature_np[i - window:i]\n",
        "        feature_new.append(feature_list.reshape(window, feature_np.shape[1]))\n",
        "        target_new.append(target_np[i+offset].reshape(1))\n",
        "    return np.array(feature_new), np.array(target_new)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:06:10.452449Z",
          "iopub.execute_input": "2022-06-30T14:06:10.452987Z",
          "iopub.status.idle": "2022-06-30T14:06:10.462591Z",
          "shell.execute_reply.started": "2022-06-30T14:06:10.452942Z",
          "shell.execute_reply": "2022-06-30T14:06:10.461234Z"
        },
        "trusted": true,
        "id": "kyk2gBQMNtjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function above is simple, but very usefull. With it we can create our time series, varying the size of the window, offset and the feature we want for our new dataset"
      ],
      "metadata": {
        "id": "ddEvBSo2NtjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scale all the dataset (not including the date)\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df.drop(columns='date'))\n",
        "df_scaled = pd.DataFrame(df_scaled, columns=df.drop(columns='date').columns)\n",
        "\n",
        "# Set the window to 10\n",
        "window = 10\n",
        "feature_columns = ['humidity', 'wind_speed', 'meanpressure', 'meantemp']\n",
        "\n",
        "# Create a window with all the columns as features (excluding the date)\n",
        "feature, target = create_window(df_scaled['meantemp'],df_scaled[feature_columns], window=window)\n",
        "print(feature[0])\n",
        "print(target[0])\n",
        "print(df_scaled.head(12))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:06:10.464302Z",
          "iopub.execute_input": "2022-06-30T14:06:10.464637Z",
          "iopub.status.idle": "2022-06-30T14:06:10.49182Z",
          "shell.execute_reply.started": "2022-06-30T14:06:10.46461Z",
          "shell.execute_reply": "2022-06-30T14:06:10.490997Z"
        },
        "trusted": true,
        "id": "17yQjgEcNtjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have our first time series, using all the columns and with a window of 10"
      ],
      "metadata": {
        "id": "_w9yYXKONtjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create train and test datasets\n",
        "def train_test(feature, target, perc_train = 0.9):\n",
        "    size_train = int(len(feature) * perc_train)\n",
        "\n",
        "    x_train = feature[0:size_train]\n",
        "    y_train = target[0:size_train]\n",
        "\n",
        "    x_test = feature[size_train: len(feature)]\n",
        "    y_test = target[size_train: len(feature)]\n",
        "\n",
        "    return x_train, x_test, y_train, y_test"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:06:10.492648Z",
          "iopub.execute_input": "2022-06-30T14:06:10.492948Z",
          "iopub.status.idle": "2022-06-30T14:06:10.499437Z",
          "shell.execute_reply.started": "2022-06-30T14:06:10.492921Z",
          "shell.execute_reply": "2022-06-30T14:06:10.498465Z"
        },
        "trusted": true,
        "id": "CZs-C4JgNtjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have a function to create the train and test dataset. We're using a custom function because we want to separate the dataset without any shuffle, just a clean cut, so we can maintain the temporal characteristic of the data"
      ],
      "metadata": {
        "id": "smb4-vxXNtjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test(feature, target)\n",
        "\n",
        "# Visualize the train and test data\n",
        "sns.lineplot(x=df['date'].iloc[window:len(y_train) + window], y=y_train[:,0], label='Train')\n",
        "sns.lineplot(x=df['date'].iloc[window + len(y_train):], y=y_test[:,0], label='Test')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:06:10.502688Z",
          "iopub.execute_input": "2022-06-30T14:06:10.503619Z",
          "iopub.status.idle": "2022-06-30T14:06:10.922929Z",
          "shell.execute_reply.started": "2022-06-30T14:06:10.503583Z",
          "shell.execute_reply": "2022-06-30T14:06:10.921653Z"
        },
        "trusted": true,
        "id": "vIjYqXFENtjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Prediction"
      ],
      "metadata": {
        "id": "mAq2r0EZNtjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a standard model using LSTM\n",
        "def model_lstm(x_shape):\n",
        "\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.LSTM(64, input_shape=(x_shape[1], x_shape[2])))\n",
        "    model.add(keras.layers.Dense(units=1))\n",
        "\n",
        "    model.compile(loss='mean_squared_error', optimizer='RMSProp')\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:06:10.924531Z",
          "iopub.execute_input": "2022-06-30T14:06:10.924899Z",
          "iopub.status.idle": "2022-06-30T14:06:10.931066Z",
          "shell.execute_reply.started": "2022-06-30T14:06:10.924866Z",
          "shell.execute_reply": "2022-06-30T14:06:10.929721Z"
        },
        "trusted": true,
        "id": "fYPkDaRZNtjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a simple model because our dataset is not too complex, so a simple neural network will do"
      ],
      "metadata": {
        "id": "lRfNwwMuNtjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we create this function because we use this same model after"
      ],
      "metadata": {
        "id": "_66MQBf-NtjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Using all the features"
      ],
      "metadata": {
        "id": "uxbfzZFbNtjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To this case test we will use all the features on our time series"
      ],
      "metadata": {
        "id": "Mwcbw9hlNtjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_lstm(x_train.shape)\n",
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:06:10.932548Z",
          "iopub.execute_input": "2022-06-30T14:06:10.9333Z",
          "iopub.status.idle": "2022-06-30T14:06:12.586211Z",
          "shell.execute_reply.started": "2022-06-30T14:06:10.933242Z",
          "shell.execute_reply": "2022-06-30T14:06:12.584841Z"
        },
        "trusted": true,
        "id": "yVA5hxxHNtjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=50)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:06:12.587681Z",
          "iopub.execute_input": "2022-06-30T14:06:12.588094Z",
          "iopub.status.idle": "2022-06-30T14:06:37.769596Z",
          "shell.execute_reply.started": "2022-06-30T14:06:12.588062Z",
          "shell.execute_reply": "2022-06-30T14:06:37.768657Z"
        },
        "trusted": true,
        "id": "ccmLTwpqNtjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the loss, our solution is pretty good!"
      ],
      "metadata": {
        "id": "Y2ewmSSxNtja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to print the results of the fit process\n",
        "def print_loss(result):\n",
        "    plt.plot(result.history['loss'])\n",
        "    plt.plot(result.history['val_loss'])\n",
        "    plt.legend(['Train', 'Test'])\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Cost')\n",
        "    plt.show()\n",
        "\n",
        "# Function to print the y_predicted compared with the y_test\n",
        "def print_test_x_prediction(y_test, y_predict, df_date, train_size, window=0):\n",
        "    sns.lineplot(x=df_date.iloc[train_size + window:], y=y_test[:,0], label = 'Test')\n",
        "    sns.lineplot(x=df_date.iloc[train_size + window:], y=y_predict[:,0], label = 'Predict')\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:06:37.772707Z",
          "iopub.execute_input": "2022-06-30T14:06:37.773203Z",
          "iopub.status.idle": "2022-06-30T14:06:37.780587Z",
          "shell.execute_reply.started": "2022-06-30T14:06:37.773156Z",
          "shell.execute_reply": "2022-06-30T14:06:37.779577Z"
        },
        "trusted": true,
        "id": "m-t3S80jNtja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict = model.predict(x_test)\n",
        "\n",
        "print_loss(result)\n",
        "print_test_x_prediction(y_test, y_predict, df['date'], len(y_train), window=window)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:06:37.781863Z",
          "iopub.execute_input": "2022-06-30T14:06:37.78284Z",
          "iopub.status.idle": "2022-06-30T14:06:38.865451Z",
          "shell.execute_reply.started": "2022-06-30T14:06:37.782793Z",
          "shell.execute_reply": "2022-06-30T14:06:38.86434Z"
        },
        "trusted": true,
        "id": "P9T7MLneNtjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, looks like our model is very precise"
      ],
      "metadata": {
        "id": "b-qMvU38Ntjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Using only the target"
      ],
      "metadata": {
        "id": "X0MyoIeLNtjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we just will use the target value on our time series and ignore all the other features"
      ],
      "metadata": {
        "id": "FkSG7IJzNtje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature, target = create_window(df_scaled['meantemp'], df_scaled[['meantemp']], window=10)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test(feature, target)\n",
        "\n",
        "model = model_lstm(x_train.shape)\n",
        "result = model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=50)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:06:38.866793Z",
          "iopub.execute_input": "2022-06-30T14:06:38.867093Z",
          "iopub.status.idle": "2022-06-30T14:07:03.712319Z",
          "shell.execute_reply.started": "2022-06-30T14:06:38.867066Z",
          "shell.execute_reply": "2022-06-30T14:07:03.710742Z"
        },
        "trusted": true,
        "id": "791vVNxbNtje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict = model.predict(x_test)\n",
        "\n",
        "print_loss(result)\n",
        "print_test_x_prediction(y_test, y_predict, df['date'], len(y_train), window=window)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:07:03.714008Z",
          "iopub.execute_input": "2022-06-30T14:07:03.71435Z",
          "iopub.status.idle": "2022-06-30T14:07:04.729024Z",
          "shell.execute_reply.started": "2022-06-30T14:07:03.71432Z",
          "shell.execute_reply": "2022-06-30T14:07:04.727779Z"
        },
        "trusted": true,
        "id": "D24iZZpKNtjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like this solution had a better result than the last one, not for much, but it's a difference"
      ],
      "metadata": {
        "id": "KvTQun8oNtjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This most likely happen because we have a simple dataset, and just the tempeture of the previous day is already enough to make a good prediction, the others features just confuse the model more than help, so less is more in this case!"
      ],
      "metadata": {
        "id": "kdWr4YB6Ntjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dense neural network\n",
        "def model_dense(x_shape):\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Dense(64, input_dim=x_shape[1], activation=nn.relu))\n",
        "    model.add(keras.layers.Dense(64,activation=nn.relu))\n",
        "    model.add(keras.layers.Dropout(0.2))\n",
        "    model.add(keras.layers.Dense(1))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:07:04.73082Z",
          "iopub.execute_input": "2022-06-30T14:07:04.731166Z",
          "iopub.status.idle": "2022-06-30T14:07:04.737803Z",
          "shell.execute_reply.started": "2022-06-30T14:07:04.731136Z",
          "shell.execute_reply": "2022-06-30T14:07:04.736605Z"
        },
        "trusted": true,
        "id": "5xRvYbY4Ntji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is to create a simple dense neural network, so we can test against our lstm model we tested early"
      ],
      "metadata": {
        "id": "an2pOHg3Ntjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Not using time series"
      ],
      "metadata": {
        "id": "TSAtDawVNtjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we don't use time series, just the features, and to help, the date separeted in month, year and day"
      ],
      "metadata": {
        "id": "bJz5Qr_zNtjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['month'] = df['date'].dt.month\n",
        "df['year'] = df['date'].dt.year\n",
        "df['day'] = df['date'].dt.day\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df.drop(columns=['date']))\n",
        "df_scaled = pd.DataFrame(df_scaled, columns=df.drop(columns=['date']).columns)\n",
        "\n",
        "feature_columns = ['humidity', 'wind_speed', 'meanpressure', 'month', 'year', 'day']\n",
        "\n",
        "feature, target = df_scaled[feature_columns], df_scaled['meantemp']\n",
        "\n",
        "feature, target = np.array(feature), np.array(target).reshape(-1,1)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test(feature, target)\n",
        "\n",
        "model = model_dense(x_train.shape)\n",
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:07:04.739567Z",
          "iopub.execute_input": "2022-06-30T14:07:04.740295Z",
          "iopub.status.idle": "2022-06-30T14:07:04.803018Z",
          "shell.execute_reply.started": "2022-06-30T14:07:04.740251Z",
          "shell.execute_reply": "2022-06-30T14:07:04.801563Z"
        },
        "trusted": true,
        "id": "UzPdzAQMNtj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=50)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:07:04.805583Z",
          "iopub.execute_input": "2022-06-30T14:07:04.805929Z",
          "iopub.status.idle": "2022-06-30T14:07:11.61908Z",
          "shell.execute_reply.started": "2022-06-30T14:07:04.805896Z",
          "shell.execute_reply": "2022-06-30T14:07:11.618169Z"
        },
        "trusted": true,
        "id": "hDMRczpMNtj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict = model.predict(x_test)\n",
        "\n",
        "print_loss(result)\n",
        "print_test_x_prediction(y_test, y_predict, df['date'], len(y_train), window=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:07:11.620837Z",
          "iopub.execute_input": "2022-06-30T14:07:11.621669Z",
          "iopub.status.idle": "2022-06-30T14:07:12.27933Z",
          "shell.execute_reply.started": "2022-06-30T14:07:11.621622Z",
          "shell.execute_reply": "2022-06-30T14:07:12.278216Z"
        },
        "trusted": true,
        "id": "fav3bnfiNtj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, is not a bad result, but nothing compared with the previous ones. This shows the power of time series for forecasting"
      ],
      "metadata": {
        "id": "o9Zfn_AJNtj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Using time series, but not with LSTM"
      ],
      "metadata": {
        "id": "frXaeoSvNtkA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we gonna use time series, but in our dense model, to see if LSTM is the one that creates the good results, or is the dataset itself"
      ],
      "metadata": {
        "id": "j6z4S9VbNtkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature, target = create_window(df_scaled['meantemp'], df_scaled[['meantemp']], window=10)\n",
        "\n",
        "feature = feature.reshape(-1, feature.shape[1] * feature.shape[2])\n",
        "x_train, x_test, y_train, y_test = train_test(feature, target)\n",
        "\n",
        "model = model_dense(x_train.shape)\n",
        "result = model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=50)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:07:12.283049Z",
          "iopub.execute_input": "2022-06-30T14:07:12.283374Z",
          "iopub.status.idle": "2022-06-30T14:07:19.024383Z",
          "shell.execute_reply.started": "2022-06-30T14:07:12.283345Z",
          "shell.execute_reply": "2022-06-30T14:07:19.023128Z"
        },
        "trusted": true,
        "id": "QFrbLV4aNtkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict = model.predict(x_test)\n",
        "\n",
        "print_loss(result)\n",
        "print_test_x_prediction(y_test, y_predict, df['date'], len(y_train), window=window)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:07:19.025979Z",
          "iopub.execute_input": "2022-06-30T14:07:19.026417Z",
          "iopub.status.idle": "2022-06-30T14:07:19.666867Z",
          "shell.execute_reply.started": "2022-06-30T14:07:19.026373Z",
          "shell.execute_reply": "2022-06-30T14:07:19.665738Z"
        },
        "trusted": true,
        "id": "FQr_CQtyNtkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow, great result, and without LSTM, this shows us that time series is more a concept than a specific algorithm, like LSTM or GRU"
      ],
      "metadata": {
        "id": "4zFjnE8cNtkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Time Series with Linear Regression"
      ],
      "metadata": {
        "id": "1z4CLWhLNtkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And to reinforce the previous idea, lets test with a simple Linear Regression and see the result"
      ],
      "metadata": {
        "id": "PIVYbBa3NtkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model_linear_reg = LinearRegression().fit(x_train, y_train)\n",
        "y_predict = model_linear_reg.predict(x_test)\n",
        "\n",
        "print_test_x_prediction(y_test, y_predict, df['date'], len(y_train), window=window)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-30T14:07:19.668247Z",
          "iopub.execute_input": "2022-06-30T14:07:19.669041Z",
          "iopub.status.idle": "2022-06-30T14:07:20.262691Z",
          "shell.execute_reply.started": "2022-06-30T14:07:19.668997Z",
          "shell.execute_reply": "2022-06-30T14:07:20.261555Z"
        },
        "trusted": true,
        "id": "t_0lbEJYNtkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, i think this proves that time series are very powerful even when not used with their most well know alghoritm"
      ],
      "metadata": {
        "id": "HVDmDAx6NtkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Conclusion"
      ],
      "metadata": {
        "id": "yBdCrH5hNtkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well this concludes this notebook, any suggestions or tips are always welcome!"
      ],
      "metadata": {
        "id": "sl4ZeeasNtkN"
      }
    }
  ]
}